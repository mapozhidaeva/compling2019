{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "morph = MorphAnalyzer()\n",
    "stops = set(stopwords.words('russian'))\n",
    "from pymystem3 import Mystem\n",
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Здесь можно прочитать краткий ход действий, а ниже посмотреть код с комментариями. \n",
    "\n",
    "\n",
    "\n",
    "## Наш бейзлайн:\n",
    "Precision - 0.13 \n",
    "\n",
    "Recall - 0.24 \n",
    "\n",
    "F1 - 0.16 \n",
    "\n",
    "Jaccard - 0.09\n",
    "\n",
    "# Результат работы:\n",
    "\n",
    "Удалось повысить F-score до 0.2\n",
    "\n",
    "# #1 Способ (failed)\n",
    "- выбрать последний вариант разбора pymorphy вместо первого + 0,01 (как выяснилось, это исключает числительные)\n",
    "\n",
    "Вот так упал результат:\n",
    "\n",
    "Precision -  0.12\n",
    "\n",
    "Recall -  0.23\n",
    "\n",
    "F1 -  0.15\n",
    "\n",
    "Jaccard -  0.09\n",
    "\n",
    "\n",
    "# #2 Cпособ 2 (success!)\n",
    "- возьмем меньше ключевых слов (6) и получим уже F-score 0.17:\n",
    "\n",
    "```sh\n",
    "keywords = [[id2word[w] for w in top] for top in texts_vectors.toarray().argsort()[:,:-6: -1]] \n",
    "\n",
    "```\n",
    "\n",
    "Precision -  0.19\n",
    "\n",
    "Recall -  0.18\n",
    "\n",
    "F1 -  0.17\n",
    "\n",
    "Jaccard -  0.11\n",
    "\n",
    "# #3 способ (success!)\n",
    "- возьмем униграммы, F-score повысился еще немного:\n",
    "\n",
    "Precision -  0.19\n",
    "\n",
    "Recall -  0.18\n",
    "\n",
    "F1 -  0.18\n",
    "\n",
    "Jaccard -  0.11\n",
    "\n",
    "# #4 способ (success!)\n",
    "- уменьшим минимальное кол-во документов: min_df=5 -> min_df=2\n",
    "\n",
    "Precision -  0.2\n",
    "\n",
    "Recall -  0.19\n",
    "\n",
    "F1 -  0.19\n",
    "\n",
    "Jaccard -  0.11\n",
    "\n",
    "# #5 Способ (success!)\n",
    "- объединить заголовок и текст и уменьшим min_df до 1 (ну да, здесь лучше сработало 1) :\n",
    "\n",
    "```sh\n",
    "data['whole_text'] = data['content_norm'] + data['title_norm']\n",
    "```\n",
    "\n",
    "Precision -  0.21\n",
    "\n",
    "Recall -  0.2\n",
    "\n",
    "F1 -  0.2\n",
    "\n",
    "Jaccard -  0.12\n",
    "\n",
    "\n",
    "\n",
    "# #6 Способ (fail)\n",
    "- я улучшила результат gensim графа тем, что разбила н-граммы на биграммы (простым циклом, потому что так и не поняла, как просить у гензима униграммы)\n",
    "В результате F-score значительно вырос (с 0.09 до 0.15), но все равно не перешагнул наш бейзлайн. Код можно посмотреть внизу.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = '/Users/Marina/PycharmProjects/compling_nlp_hse_course/ru_kw_eval_datasets/data/ng'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [os.path.join(PATH_TO_DATA, file) for file in os.listdir(PATH_TO_DATA)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Marina/PycharmProjects/compling_nlp_hse_course/ru_kw_eval_datasets/data/ng/ng_1.jsonlines.zip'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = files[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(files[2], lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(988, 5)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(true_kws, predicted_kws):\n",
    "    assert len(true_kws) == len(predicted_kws)\n",
    "    \n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    jaccards = []\n",
    "    \n",
    "    for i in range(len(true_kws)):\n",
    "        true_kw = set(true_kws[i])\n",
    "        predicted_kw = set(predicted_kws[i])\n",
    "        \n",
    "        tp = len(true_kw & predicted_kw)\n",
    "        union = len(true_kw | predicted_kw)\n",
    "        fp = len(predicted_kw - true_kw)\n",
    "        fn = len(true_kw - predicted_kw)\n",
    "        \n",
    "        if (tp+fp) == 0:\n",
    "            prec = 0\n",
    "        else:\n",
    "            prec = tp / (tp + fp)\n",
    "        \n",
    "        if (tp+fn) == 0:\n",
    "            rec = 0\n",
    "        else:\n",
    "            rec = tp / (tp + fn)\n",
    "        if (prec+rec) == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = (2*(prec*rec))/(prec+rec)\n",
    "            \n",
    "        jac = tp / union\n",
    "        \n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        f1s.append(f1)\n",
    "        jaccards.append(jac)\n",
    "    print('Precision - ', round(np.mean(precisions), 2))\n",
    "    print('Recall - ', round(np.mean(recalls), 2))\n",
    "    print('F1 - ', round(np.mean(f1s), 2))\n",
    "    print('Jaccard - ', round(np.mean(jaccards), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Здесь я неудачно пыталась брать последний разбор пайморфи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "stops = set(stopwords.words('russian'))\n",
    "\n",
    "def normalize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0] for word in words if word and word not in stops]\n",
    "    words = [word.normal_form for word in words if word.tag.POS == 'NOUN']\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#morph.parse('привет я марина у меня есть сто рублей')\n",
    "analysis = morph.parse('сто')\n",
    "for i, word in enumerate(analysis):\n",
    "    if word.tag.POS == 'NUMR': #.normal_form()\n",
    "        print (word.tag.POS)\n",
    "        print (analysis[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m.analyze('иванов')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Surn' in morph.parse('сто')[0][1]\n",
    "#morph.parse('двести')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content_norm'] = data['content'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['title_norm'] = data['title'].apply(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Вот здесь я объединила текст и заголовок:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['whole_text'] = data['content_norm'] + data['title_norm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kw1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Здесь поменяла биграммы на униграммы и уменьшина min_df (мин кол-во документов, в кот встречается слово) сначала до 2, потом до 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1, 1), min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['content_norm_str'] = data['content_norm'].apply(' '.join)\n",
    "data['whole_text_norm_str'] = data['whole_text'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 688,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tfidf.fit(data['content_norm_str'])\n",
    "tfidf.fit(data['whole_text_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = {i:word for i,word in enumerate(tfidf.get_feature_names())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [],
   "source": [
    "#texts_vectors = tfidf.transform(data['content_norm_str'])\n",
    "texts_vectors = tfidf.transform(data['whole_text_str'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Здесь уменьшила кол-во ключевых слов с 10 до 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [[id2word[w] for w in top] for top in texts_vectors.toarray().argsort()[:,:-6: -1]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.21\n",
      "Recall -  0.2\n",
      "F1 -  0.2\n",
      "Jaccard -  0.12\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вот тут я разбила н-граммы на униграммы (способ 6):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Precision -  0.08\n",
    "Recall -  0.11\n",
    "F1 -  0.08\n",
    "Jaccard -  0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Precision -  0.12\n",
    "Recall -  0.24\n",
    "F1 -  0.15\n",
    "Jaccard -  0.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_kws = data['whole_text'].apply(lambda x: keywords(' '.join(x)).split('\\n')[:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.08\n",
      "Recall -  0.1\n",
      "F1 -  0.09\n",
      "Jaccard -  0.05\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], gensim_kws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_kws2 = []\n",
    "for i in gensim_kws:\n",
    "    i = ' '.join([w for w in i])\n",
    "    gensim_kws2.append(i.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.12\n",
      "Recall -  0.23\n",
      "F1 -  0.15\n",
      "Jaccard -  0.09\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], gensim_kws2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
